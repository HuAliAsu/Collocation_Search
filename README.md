# ابزار پیشرفته تحلیل و جستجوی متن فارسی

این یک برنامه دسکتاپ (Desktop Application) قدرتمند است که با پایتون و کتابخانه Tkinter توسعه داده شده و برای تحلیلگران متن، زبان‌شناسان و پژوهشگران طراحی شده است. این ابزار به کاربران اجازه می‌دهد تا مجموعه‌ای بزرگ از متون (Corpus) را که در فایل‌های `.docx` قرار دارند، پردازش کرده و جستجوهای پیچیده‌ای بر اساس کلمات هم‌نشین (Collocations) و نقش دستوری آن‌ها (Part-of-Speech) انجام دهند.

این پروژه به عنوان یک نمونه کامل از ترکیب پردازش زبان طبیعی (NLP) با رابط کاربری گرافیکی (GUI) در پایتون عمل می‌کند.

-----

## ویژگی‌های کلیدی

  * **پردازش دسته‌ای فایل‌های متنی:** قابلیت خواندن و پردازش تمام فایل‌های `.docx` موجود در یک پوشه و زیرپوشه‌های آن.
  * **اصلاح هوشمند متن:** امکان اعمال یک لیست اصلاحات سفارشی (از طریق فایل اکسل) برای تصحیح غلط‌های املایی رایج در کل مجموعه متون.
  * **پردازش هوشمند پاراگراف:** منطق پیشرفته برای ادغام پاراگراف‌های ناقص (که به نقطه ختم نمی‌شوند) و شکستن پاراگراف‌های بسیار طولانی از محل پایان جملات برای استانداردسازی داده‌ها.
  * **برچسب‌گذاری نقش دستوری (POS Tagging):** استفاده از کتابخانه `hazm` برای تحلیل دستوری جملات و تشخیص اجزای کلام (اسم، فعل، صفت و...).
  * **کش (Cache) کردن داده‌ها:** پس از اولین پردازش که ممکن است زمان‌بر باشد، نتایج در یک فایل `.pkl` ذخیره می‌شوند. این ویژگی باعث می‌شود برنامه در اجراهای بعدی تقریباً بلافاصله و با سرعت بسیار بالا بارگذاری شود.
  * **جستجوی پیشرفته و چندوجهی:**
      * جستجو بر اساس یک عبارت دقیق (چند کلمه‌ای).
      * تحلیل کلمات قبل و بعد از عبارت کلیدی.
      * فیلتر کردن نتایج بر اساس یک کلمه خاص یا شروع یک کلمه.
      * فیلتر کردن نتایج بر اساس نقش دستوری کلمه (مثلاً یافتن تمام اسم‌هایی که بعد از عبارت کلیدی آمده‌اند).
  * **رابط کاربری تعاملی:**
      * نمایش نتایج در یک جدول قابل مرتب‌سازی (Sortable).
      * نمایش جملات منبع به همراه نام کتاب برای هر نتیجه.
      * پشتیبانی کامل از نمایش صحیح متون راست‌به‌چپ (RTL) حتی در حالت‌های پیچیده.
      * دکمه "پردازش مجدد" برای به‌روزرسانی داده‌ها در صورت افزودن کتاب‌های جدید.

## تکنولوژی‌های استفاده شده

  * **زبان برنامه‌نویسی:** Python 3
  * **رابط کاربری گرافیکی:** `tkinter` (به همراه ویجت‌های مدرن `ttk`)
  * **پردازش زبان طبیعی:** `hazm`
  * **کار با فایل‌ها:**
      * `pandas` و `openpyxl` برای خواندن فایل اکسل لیست اصلاحات.
      * `python-docx` برای خواندن فایل‌های Word.
      * `pickle` برای ذخیره و بازیابی فایل کش.

## پیش‌نیازها

برای اجرای این برنامه به موارد زیر نیاز دارید:

  * Python (نسخه 3.8 یا بالاتر)
  * `pip` (ابزار مدیریت بسته‌های پایتون که معمولاً به همراه پایتون نصب می‌شود)

## نصب و راه‌اندازی

1.  **کلون کردن ریپازیتوری:**

    ```sh
    git clone https://github.com/your-username/your-repository-name.git
    cd your-repository-name
    ```

2.  **(توصیه‌شده) ساخت یک محیط مجازی:**

    ```sh
    # ساخت محیط مجازی
    python -m venv venv
    # فعال‌سازی در ویندوز
    venv\Scripts\activate
    # فعال‌سازی در لینوکس یا macOS
    source venv/bin/activate
    ```

3.  **نصب کتابخانه‌های مورد نیاز:**
    یک فایل با نام `requirements.txt` در پوشه اصلی پروژه بسازید و محتوای زیر را در آن کپی کنید:

    ```txt
    pandas
    hazm
    python-docx
    openpyxl
    ```

    سپس دستور زیر را برای نصب این کتابخانه‌ها اجرا کنید:

    ```sh
    pip install -r requirements.txt
    ```

4.  **قرار دادن مدل `hazm`:**
    فایل مدل `pos_tagger.model` که مربوط به کتابخانه `hazm` است را دانلود کرده و در **کنار فایل اصلی برنامه (`analyzer_app.py`)** قرار دهید.

## نحوه استفاده

1.  **اجرای برنامه:**

    ```sh
    python analyzer_app.py
    ```

2.  **پردازش اولیه (فقط برای بار اول):**

      * در اولین اجرا، برنامه تشخیص می‌دهد که فایل کش (`preprocessed_data.pkl`) وجود ندارد.
      * ابتدا یک پنجره برای انتخاب **پوشه حاوی کتاب‌ها** باز می‌شود. پوشه مورد نظر را انتخاب کنید.
      * سپس پنجره دیگری برای انتخاب **فایل اکسل لیست اصلاحات** باز می‌شود (این مرحله اختیاری است).
      * برنامه شروع به پردازش تمام فایل‌ها می‌کند. این فرآیند ممکن است بسته به حجم داده‌های شما چند دقیقه طول بکشد. لطفاً تا پایان آن صبور باشید.
      * پس از اتمام، فایل کش `preprocessed_data.pkl` به صورت خودکار ساخته می‌شود.

3.  **اجراهای بعدی:**

      * از این به بعد، برنامه با خواندن مستقیم فایل کش، در عرض چند ثانیه و با سرعت بالا اجرا خواهد شد.

4.  **جستجو و تحلیل:**

      * از ابزارهای موجود در بالای صفحه برای انجام جستجوهای خود استفاده کنید.
      * با کلیک بر روی هر نتیجه در جدول، می‌توانید جملات منبع آن را در کادر پایینی مشاهده کنید.
      * اگر کتاب‌های جدیدی به پوشه خود اضافه کردید، از دکمه **"پردازش مجدد"** برای به‌روزرسانی کامل پایگاه داده استفاده کنید.

## ساختار فایل‌ها

```
/your-repository-name
|-- analyzer_app.py         # اسکریپت اصلی برنامه
|-- pos_tagger.model        # فایل مدل Hazm (باید در اینجا کپی شود چون متاسفانه با کتابخانه صلی لود نشد) .
|-- requirements.txt        # لیست کتابخانه‌های مورد نیاز
|-- preprocessed_data.pkl   # فایل کش (پس از اولین اجرا ساخته می‌شود)
`-- README.md               # همین فایل توضیحات
```

## مجوز (License)

این پروژه تحت مجوز MIT منتشر شده است. برای اطلاعات بیشتر فایل `LICENSE` را مطالعه کنید.
*(توصیه می‌شود یک فایل به نام `LICENSE` ساخته و متن مجوز MIT را در آن قرار دهید.)*